{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Explanation\n",
    "\n",
    "**trainRNN_HBTRC_ANM_AttentionModel.ipynb:**\n",
    "<br> This notebook is to load AddNeuroMed examples from 'preprocessData.pickle', and HBTRC examples from 'preprocessData_HBTRC.pickle' and create an \"Attention Network\". Train the network on HBTRC data set and test on AddNeuroMed data set\n",
    "\n",
    "**Variables information**:\n",
    "<br> 1) Variables in the format of xxx_A represents data from AddNeuroMed\n",
    "<br> 2) Variables in the format of xxx_H represents data from HBTRC\n",
    "\n",
    "**Processes are as follows:**\n",
    "<br> 1) Load all variables from 'preprocessData.pickle' and 'preprocessData_HBTRC.pickle'\n",
    "<br> 2) Parameter and hyperparameter assignments (location: **3rd cell**)\n",
    "<br> 3) Create LSTM cells with Dropout Wrappers for gene A and gene B (function: **dropoutWrapper** in **trainRNN_network_utils.py**)\n",
    "<br> 4) Using LSTM cells, create multi-layer dynamic model from fixed length sequences (function: **dynamicLSTM_Attention** in **trainRNN_network_utils.py**)\n",
    "<br> 5) Create an attention mechanism based on a fully-connected layer of states and output, which is followed by a tanh layer to calculate score. Then, calculate attention weights and context vector using softmax and dense layers \n",
    "<br> 6) Create a single output from a concatenation of context vectors of gene A and gene B\n",
    "<br> 7) Pass the output through a **dense** layer and make prediction\n",
    "<br> 8) Before starting the training: concatenate rSnpG_tr_nXSN and rRnaG_nXS where G represents gene A and gene B (function: **input_reshape** in **trainRNN_utils.py**)\n",
    "<br> 9) Train the network: every epoch (i.e., iteration) shuffle the data within each class (function: **shuffle_classes** in **trainRNN_utils.py**) and train in batches (function: **extract_batch_size** in **trainRNN_utils.py**)\n",
    "<br> 10) Plot results with **plot_one_input** in **trainRNN_plot_utils.py**)\n",
    "<br> 11) Save them in \"resultsAttention_HBTRC_ANM.pickle\" to be called when necessary\n",
    "\n",
    "**Variables created:**\n",
    "<br> 1) **trainLosses**: Train losses, dictionary, keys of (dropout)\n",
    "<br> 2) **testLosses**: Test losses, dictionary, keys of (dropout)\n",
    "<br> 3) **F1_scores**: F1_scores, dictionary, keys of (dropout)\n",
    "<br> 4) **trainAccuracy**: Train accuracy, dictionary, keys of (dropout)\n",
    "<br> 5) **attention_matrixA**: Attention weights of gene A, dictionary, keys of (dropout)\n",
    "<br> 6) **attention_matrixB**: Attention weights of gene B, dictionary, keys of (dropout)\n",
    "<br> 7) **tst_prediction**: Test predictions, dictionary, keys of (dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.core.debugger import set_trace #set_trace()\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import rnn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[ \"CUDA_VISIBLE_DEVICES\" ] = \"3\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddNeuroMed data loaded from pickle.\n",
      "All AddNeuroMed samples loaded.\n",
      "Number of training samples (transcripts) of gene A: 1500\n",
      "Number of training samples (transcripts) of gene B: 1500\n",
      "Number of test samples (transcripts) of gene A: 45\n",
      "Number of test samples (transcripts) of gene B: 45\n",
      "Number of subjects iSnum: 206\n",
      "Number of SNPs iNnum: 100\n",
      "Number of association classes iCnum: 3\n"
     ]
    }
   ],
   "source": [
    "# LOAD AddNeuroMed DATA\n",
    "# Load data form the pickle produced by \"preprocessData.ipynb\" in AddNeuroMed folder\n",
    "\n",
    "# Save data into Python file\n",
    "import pickle\n",
    "with open('../AddNeuroMed/preprocessData.pickle', 'rb') as f:\n",
    "    rSnpA_nXSN_A = pickle.load( f )\n",
    "    rSnpB_nXSN_A = pickle.load( f )\n",
    "    rRnaA_nXS_A = pickle.load( f )\n",
    "    rRnaB_nXS_A = pickle.load( f )\n",
    "    rRelated_nXC_A = pickle.load( f )\n",
    "    rSnpA_tr_nXSN_A = pickle.load( f )\n",
    "    rSnpB_tr_nXSN_A = pickle.load( f )\n",
    "    rRnaA_tr_nXS_A = pickle.load( f )\n",
    "    rRnaB_tr_nXS_A = pickle.load( f )\n",
    "    rRelated_tr_nXC_A = pickle.load( f )\n",
    "    rSnpA_tst_nXSN_A = pickle.load( f )\n",
    "    rSnpB_tst_nXSN_A = pickle.load( f )\n",
    "    rRnaA_tst_nXS_A = pickle.load( f )\n",
    "    rRnaB_tst_nXS_A = pickle.load( f )\n",
    "    rRelated_tst_nXC_A = pickle.load( f )\n",
    "    sGeneNames_nX2_A = pickle.load( f )\n",
    "    nRs_A = pickle.load( f )\n",
    "    nSs_A = pickle.load( f )\n",
    "    print( 'AddNeuroMed data loaded from pickle.' )\n",
    "\n",
    "\n",
    "# Check the input dimensions\n",
    "assert( len( rSnpA_nXSN_A.shape ) == 3 )\n",
    "assert( len( rSnpB_nXSN_A.shape ) == 3 )\n",
    "assert( len( rRnaA_nXS_A.shape ) == 2 )\n",
    "assert( len( rRnaB_nXS_A.shape ) == 2)\n",
    "assert( len( rRelated_nXC_A.shape ) == 2 )\n",
    "assert( len( rSnpA_tr_nXSN_A.shape ) == 3 )\n",
    "assert( len( rSnpB_tr_nXSN_A.shape ) == 3 )\n",
    "assert( len( rRnaA_tr_nXS_A.shape ) == 2 )\n",
    "assert( len( rRnaB_tr_nXS_A.shape ) == 2 )\n",
    "assert( len( rRelated_tr_nXC_A.shape ) == 2 )\n",
    "assert( len( rSnpA_tst_nXSN_A.shape ) == 3 )\n",
    "assert( len( rSnpB_tst_nXSN_A.shape ) == 3 )\n",
    "assert( len( rRnaA_tst_nXS_A.shape ) == 2 )\n",
    "assert( len( rRnaB_tst_nXS_A.shape ) == 2 )\n",
    "assert( len( rRelated_tst_nXC_A.shape ) == 2)\n",
    "assert( rSnpA_nXSN_A.shape[ 0 ] == rRnaA_nXS_A.shape[0] )\n",
    "assert( rSnpA_nXSN_A.shape[ 0 ] == rRnaB_nXS_A.shape[0] )\n",
    "assert( rSnpB_nXSN_A.shape[ 0 ] == rRnaA_nXS_A.shape[0] )\n",
    "assert( rSnpB_nXSN_A.shape[ 0 ] == rRnaB_nXS_A.shape[0] )\n",
    "assert( rSnpA_nXSN_A.shape[ 0 ] == rRelated_nXC_A.shape[ 0 ] )\n",
    "assert( rSnpA_nXSN_A.shape[ 1 ] == rRnaA_nXS_A.shape[ 1 ] )\n",
    "assert( rSnpB_nXSN_A.shape[ 1 ] == rRnaB_nXS_A.shape[ 1 ] )\n",
    "assert( rRelated_nXC_A.shape[ 1 ] == 3 )\n",
    "\n",
    "iSnum_A = rSnpA_nXSN_A.shape[ 1 ] # Number of subjects\n",
    "iNnum_A = rSnpA_nXSN_A.shape[ 2 ] # Number of snps\n",
    "iCnum_A = rRelated_nXC_A.shape[ 1 ] # Number of classes\n",
    "\n",
    "print('All AddNeuroMed samples loaded.' )\n",
    "print('Number of training samples (transcripts) of gene A: {}'.format( rSnpA_tr_nXSN_A.shape[ 0 ] ) )\n",
    "print('Number of training samples (transcripts) of gene B: {}'.format( rSnpB_tr_nXSN_A.shape[ 0 ] ) )\n",
    "print('Number of test samples (transcripts) of gene A: {}'.format( rSnpA_tst_nXSN_A.shape[ 0 ] ) )\n",
    "print('Number of test samples (transcripts) of gene B: {}'.format( rSnpB_tst_nXSN_A.shape[ 0 ] ) )\n",
    "print('Number of subjects iSnum: {}'.format( rSnpA_nXSN_A.shape[ 1 ] ) )\n",
    "print('Number of SNPs iNnum: {}'.format( rSnpA_nXSN_A.shape[ 2 ] ) )\n",
    "print('Number of association classes iCnum: {}'.format( rRelated_nXC_A.shape[ 1 ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HBTRC data loaded from pickle.\n",
      "All HBTRC samples loaded.\n",
      "Number of training samples (transcripts) of gene A: 1500\n",
      "Number of training samples (transcripts) of gene B: 1500\n",
      "Number of test samples (transcripts) of gene A: 45\n",
      "Number of test samples (transcripts) of gene B: 45\n",
      "Number of subjects iSnum: 434\n",
      "Number of SNPs iNnum: 100\n",
      "Number of association classes iCnum: 3\n"
     ]
    }
   ],
   "source": [
    "# LOAD HBTRC DATA\n",
    "# Load data form the pickle produced by \"preprocessData_HBTRC.ipynb\"\n",
    "\n",
    "# Save data into Python file\n",
    "import pickle\n",
    "with open('../HBTRC/preprocessData_HBTRC.pickle', 'rb') as f:\n",
    "    rSnpA_nXSN_H = pickle.load( f )\n",
    "    rSnpB_nXSN_H = pickle.load( f )\n",
    "    rRnaA_nXS_H = pickle.load( f )\n",
    "    rRnaB_nXS_H = pickle.load( f )\n",
    "    rRelated_nXC_H = pickle.load( f )\n",
    "    rSnpA_tr_nXSN_H = pickle.load( f )\n",
    "    rSnpB_tr_nXSN_H = pickle.load( f )\n",
    "    rRnaA_tr_nXS_H = pickle.load( f )\n",
    "    rRnaB_tr_nXS_H = pickle.load( f )\n",
    "    rRelated_tr_nXC_H = pickle.load( f )\n",
    "    rSnpA_tst_nXSN_H = pickle.load( f )\n",
    "    rSnpB_tst_nXSN_H = pickle.load( f )\n",
    "    rRnaA_tst_nXS_H = pickle.load( f )\n",
    "    rRnaB_tst_nXS_H = pickle.load( f )\n",
    "    rRelated_tst_nXC_H = pickle.load( f )\n",
    "    sGeneNames_nX2_H = pickle.load( f )\n",
    "    nRs_H = pickle.load( f )\n",
    "    nSs_H = pickle.load( f )\n",
    "    print( 'HBTRC data loaded from pickle.' )\n",
    "\n",
    "\n",
    "# Check the input dimensions\n",
    "assert( len( rSnpA_nXSN_H.shape ) == 3 )\n",
    "assert( len( rSnpB_nXSN_H.shape ) == 3 )\n",
    "assert( len( rRnaA_nXS_H.shape ) == 2 )\n",
    "assert( len( rRnaB_nXS_H.shape ) == 2)\n",
    "assert( len( rRelated_nXC_H.shape ) == 2 )\n",
    "assert( len( rSnpA_tr_nXSN_H.shape ) == 3 )\n",
    "assert( len( rSnpB_tr_nXSN_H.shape ) == 3 )\n",
    "assert( len( rRnaA_tr_nXS_H.shape ) == 2 )\n",
    "assert( len( rRnaB_tr_nXS_H.shape ) == 2 )\n",
    "assert( len( rRelated_tr_nXC_H.shape ) == 2 )\n",
    "assert( len( rSnpA_tst_nXSN_H.shape ) == 3 )\n",
    "assert( len( rSnpB_tst_nXSN_H.shape ) == 3 )\n",
    "assert( len( rRnaA_tst_nXS_H.shape ) == 2 )\n",
    "assert( len( rRnaB_tst_nXS_H.shape ) == 2 )\n",
    "assert( len( rRelated_tst_nXC_H.shape ) == 2)\n",
    "assert( rSnpA_nXSN_H.shape[ 0 ] == rRnaA_nXS_H.shape[0] )\n",
    "assert( rSnpA_nXSN_H.shape[ 0 ] == rRnaB_nXS_H.shape[0] )\n",
    "assert( rSnpB_nXSN_H.shape[ 0 ] == rRnaA_nXS_H.shape[0] )\n",
    "assert( rSnpB_nXSN_H.shape[ 0 ] == rRnaB_nXS_H.shape[0] )\n",
    "assert( rSnpA_nXSN_H.shape[ 0 ] == rRelated_nXC_H.shape[ 0 ] )\n",
    "assert( rSnpA_nXSN_H.shape[ 1 ] == rRnaA_nXS_H.shape[ 1 ] )\n",
    "assert( rSnpB_nXSN_H.shape[ 1 ] == rRnaB_nXS_H.shape[ 1 ] )\n",
    "assert( rRelated_nXC_H.shape[ 1 ] == 3 )\n",
    "\n",
    "iSnum_H = rSnpA_nXSN_H.shape[ 1 ] # Number of subjects\n",
    "iNnum_H = rSnpA_nXSN_H.shape[ 2 ] # Number of snps\n",
    "iCnum_H = rRelated_nXC_H.shape[ 1 ] # Number of classes\n",
    "\n",
    "print('All HBTRC samples loaded.' )\n",
    "print('Number of training samples (transcripts) of gene A: {}'.format( rSnpA_tr_nXSN_H.shape[ 0 ] ) )\n",
    "print('Number of training samples (transcripts) of gene B: {}'.format( rSnpB_tr_nXSN_H.shape[ 0 ] ) )\n",
    "print('Number of test samples (transcripts) of gene A: {}'.format( rSnpA_tst_nXSN_H.shape[ 0 ] ) )\n",
    "print('Number of test samples (transcripts) of gene B: {}'.format( rSnpB_tst_nXSN_H.shape[ 0 ] ) )\n",
    "print('Number of subjects iSnum: {}'.format( rSnpA_nXSN_H.shape[ 1 ] ) )\n",
    "print('Number of SNPs iNnum: {}'.format( rSnpA_nXSN_H.shape[ 2 ] ) )\n",
    "print('Number of association classes iCnum: {}'.format( rRelated_nXC_H.shape[ 1 ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "## Input data\n",
    "time_steps = iNnum_H + 1                            # number of snps + number of rnas\n",
    "n_input = iSnum_H                                   # number of subjects\n",
    "\n",
    "## LSTM's internal structure\n",
    "n_hidden = 32                                       # number of nodes in hidden layer \n",
    "n_classes = iCnum_H                                 # number of classes\n",
    "n_layer = 3                                         # number of layers\n",
    "dropout = 0.5                                       # dropout percentage\n",
    "\n",
    "## Training data\n",
    "learning_rate = 0.001\n",
    "batch_size = 150\n",
    "n_epoch = 250\n",
    "n_batch = rSnpA_tr_nXSN_H.shape[0] // batch_size # number of batches\n",
    "lambda_l2_reg = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gulkamisli/Google Drive /Oxford/MSc Dissertation 2019/MSc_Dissertation2019_1019089/Source Code/AddNeuroMed+HBTRC/trainRNN_network_utils.py:15: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/gulkamisli/Google Drive /Oxford/MSc Dissertation 2019/MSc_Dissertation2019_1019089/Source Code/AddNeuroMed+HBTRC/trainRNN_network_utils.py:70: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/gulkamisli/Google Drive /Oxford/MSc Dissertation 2019/MSc_Dissertation2019_1019089/Source Code/AddNeuroMed+HBTRC/trainRNN_network_utils.py:74: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/gulkamisli/Google Drive /Oxford/MSc Dissertation 2019/MSc_Dissertation2019_1019089/Source Code/AddNeuroMed+HBTRC/trainRNN_network_utils.py:99: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:1472: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:2176: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Data shuffled. Epoch:  0\n",
      "Performance on training data: Loss = 1.1179763078689575: Accuracy = 0.41333332657814026\n",
      "Performance on test set: : Loss = 1.2517797946929932: Accuracy = 0.5521235560761587\n",
      "\n",
      "Data shuffled. Epoch:  1\n",
      "Performance on training data: Loss = 1.1211689710617065: Accuracy = 0.4466666579246521\n",
      "Performance on test set: : Loss = 1.2349066734313965: Accuracy = 0.5740236286477001\n",
      "\n",
      "Data shuffled. Epoch:  2\n",
      "Performance on training data: Loss = 1.0646649599075317: Accuracy = 0.47999998927116394\n",
      "Performance on test set: : Loss = 1.134616732597351: Accuracy = 0.6222732587064438\n",
      "\n",
      "Data shuffled. Epoch:  3\n",
      "Performance on training data: Loss = 0.9148685932159424: Accuracy = 0.5533333420753479\n",
      "Performance on test set: : Loss = 1.1731767654418945: Accuracy = 0.6505016408444616\n",
      "\n",
      "Data shuffled. Epoch:  4\n",
      "Performance on training data: Loss = 1.0166324377059937: Accuracy = 0.4466666579246521\n",
      "Performance on test set: : Loss = 1.2187232971191406: Accuracy = 0.6519073738268376\n",
      "\n",
      "Data shuffled. Epoch:  5\n",
      "Performance on training data: Loss = 0.733922004699707: Accuracy = 0.5733333230018616\n",
      "Performance on test set: : Loss = 1.3295315504074097: Accuracy = 0.6375036472298425\n",
      "\n",
      "Data shuffled. Epoch:  6\n",
      "Performance on training data: Loss = 0.7999293804168701: Accuracy = 0.5333333611488342\n",
      "Performance on test set: : Loss = 1.2476861476898193: Accuracy = 0.6365007549233624\n",
      "\n",
      "Data shuffled. Epoch:  7\n",
      "Performance on training data: Loss = 0.6877713799476624: Accuracy = 0.5933333039283752\n",
      "Performance on test set: : Loss = 1.6092151403427124: Accuracy = 0.6315555786018198\n",
      "\n",
      "Data shuffled. Epoch:  8\n",
      "Performance on training data: Loss = 0.6982638835906982: Accuracy = 0.5666666626930237\n",
      "Performance on test set: : Loss = 1.6076112985610962: Accuracy = 0.6345017846390346\n",
      "\n",
      "Data shuffled. Epoch:  9\n",
      "Performance on training data: Loss = 0.6586236357688904: Accuracy = 0.6466666460037231\n",
      "Performance on test set: : Loss = 1.6589792966842651: Accuracy = 0.6312600078119391\n",
      "\n",
      "Data shuffled. Epoch:  10\n",
      "Performance on training data: Loss = 0.9440984129905701: Accuracy = 0.4866666793823242\n",
      "Performance on test set: : Loss = 1.5621370077133179: Accuracy = 0.6300925404995442\n",
      "\n",
      "Data shuffled. Epoch:  11\n",
      "Performance on training data: Loss = 0.8352546691894531: Accuracy = 0.5733333230018616\n",
      "Performance on test set: : Loss = 1.5646029710769653: Accuracy = 0.6266786002225017\n",
      "\n",
      "Data shuffled. Epoch:  12\n",
      "Performance on training data: Loss = 0.6684509515762329: Accuracy = 0.5933333039283752\n",
      "Performance on test set: : Loss = 1.587544560432434: Accuracy = 0.624309364704845\n",
      "\n",
      "Data shuffled. Epoch:  13\n",
      "Performance on training data: Loss = 0.6096224784851074: Accuracy = 0.6399999856948853\n",
      "Performance on test set: : Loss = 1.7368746995925903: Accuracy = 0.6227822394069419\n",
      "\n",
      "Data shuffled. Epoch:  14\n",
      "Performance on training data: Loss = 0.6312154531478882: Accuracy = 0.6800000071525574\n",
      "Performance on test set: : Loss = 1.82552170753479: Accuracy = 0.6217255368795231\n",
      "\n",
      "Data shuffled. Epoch:  15\n",
      "Performance on training data: Loss = 0.6486103534698486: Accuracy = 0.5799999833106995\n",
      "Performance on test set: : Loss = 2.1003129482269287: Accuracy = 0.621926467525107\n",
      "\n",
      "Data shuffled. Epoch:  16\n",
      "Performance on training data: Loss = 0.5965131521224976: Accuracy = 0.6266666650772095\n",
      "Performance on test set: : Loss = 2.3325600624084473: Accuracy = 0.6223686860744967\n",
      "\n",
      "Data shuffled. Epoch:  17\n",
      "Performance on training data: Loss = 0.5841525793075562: Accuracy = 0.6399999856948853\n",
      "Performance on test set: : Loss = 2.148958683013916: Accuracy = 0.6227629378382352\n",
      "\n",
      "Data shuffled. Epoch:  18\n",
      "Performance on training data: Loss = 0.6425092816352844: Accuracy = 0.6133333444595337\n",
      "Performance on test set: : Loss = 2.2329509258270264: Accuracy = 0.6231165366966288\n",
      "\n",
      "Data shuffled. Epoch:  19\n",
      "Performance on training data: Loss = 0.5625903010368347: Accuracy = 0.6266666650772095\n",
      "Performance on test set: : Loss = 2.6006968021392822: Accuracy = 0.6234355243027292\n",
      "\n",
      "Data shuffled. Epoch:  20\n",
      "Performance on training data: Loss = 0.575623095035553: Accuracy = 0.653333306312561\n",
      "Performance on test set: : Loss = 2.571758270263672: Accuracy = 0.6237247492765988\n",
      "\n",
      "Data shuffled. Epoch:  21\n",
      "Performance on training data: Loss = 0.5550660490989685: Accuracy = 0.6600000262260437\n",
      "Performance on test set: : Loss = 2.4255306720733643: Accuracy = 0.6238744027622265\n",
      "\n",
      "Data shuffled. Epoch:  22\n",
      "Performance on training data: Loss = 0.7959769368171692: Accuracy = 0.6133333444595337\n",
      "Performance on test set: : Loss = 2.7287840843200684: Accuracy = 0.62412024630639\n",
      "\n",
      "Data shuffled. Epoch:  23\n",
      "Performance on training data: Loss = 1.049438714981079: Accuracy = 0.47999998927116394\n",
      "Performance on test set: : Loss = 2.366190195083618: Accuracy = 0.6219688637298412\n",
      "\n",
      "Data shuffled. Epoch:  24\n",
      "Performance on training data: Loss = 0.8327555060386658: Accuracy = 0.6000000238418579\n",
      "Performance on test set: : Loss = 1.9892079830169678: Accuracy = 0.6207854676201193\n",
      "\n",
      "Data shuffled. Epoch:  25\n",
      "Performance on training data: Loss = 0.6793120503425598: Accuracy = 0.6399999856948853\n",
      "Performance on test set: : Loss = 1.8642903566360474: Accuracy = 0.6207333610181331\n",
      "\n",
      "Data shuffled. Epoch:  26\n",
      "Performance on training data: Loss = 0.6613079905509949: Accuracy = 0.5933333039283752\n",
      "Performance on test set: : Loss = 1.805004596710205: Accuracy = 0.6212275355152806\n",
      "\n",
      "Data shuffled. Epoch:  27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training data: Loss = 0.6317282915115356: Accuracy = 0.653333306312561\n",
      "Performance on test set: : Loss = 2.0121328830718994: Accuracy = 0.6215243566351162\n",
      "\n",
      "Data shuffled. Epoch:  28\n",
      "Performance on training data: Loss = 0.6183903813362122: Accuracy = 0.5600000023841858\n",
      "Performance on test set: : Loss = 2.2374234199523926: Accuracy = 0.6218009943217435\n",
      "\n",
      "Data shuffled. Epoch:  29\n",
      "Performance on training data: Loss = 0.5798348188400269: Accuracy = 0.6399999856948853\n",
      "Performance on test set: : Loss = 2.4238500595092773: Accuracy = 0.62205930628022\n",
      "\n",
      "Data shuffled. Epoch:  30\n",
      "Performance on training data: Loss = 0.5658764243125916: Accuracy = 0.6666666865348816\n",
      "Performance on test set: : Loss = 2.645427703857422: Accuracy = 0.6223010885976212\n",
      "\n",
      "Data shuffled. Epoch:  31\n",
      "Performance on training data: Loss = 0.5252750515937805: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 2.5392208099365234: Accuracy = 0.6225279134917439\n",
      "\n",
      "Data shuffled. Epoch:  32\n",
      "Performance on training data: Loss = 0.5850446820259094: Accuracy = 0.6133333444595337\n",
      "Performance on test set: : Loss = 2.644172191619873: Accuracy = 0.6227065381368563\n",
      "\n",
      "Data shuffled. Epoch:  33\n",
      "Performance on training data: Loss = 0.5868827104568481: Accuracy = 0.6333333253860474\n",
      "Performance on test set: : Loss = 2.6641972064971924: Accuracy = 0.6228074848366161\n",
      "\n",
      "Data shuffled. Epoch:  34\n",
      "Performance on training data: Loss = 0.5561301112174988: Accuracy = 0.6266666650772095\n",
      "Performance on test set: : Loss = 2.7129759788513184: Accuracy = 0.622844260511262\n",
      "\n",
      "Data shuffled. Epoch:  35\n",
      "Performance on training data: Loss = 0.5851854681968689: Accuracy = 0.6733333468437195\n",
      "Performance on test set: : Loss = 2.7966065406799316: Accuracy = 0.6228539297810716\n",
      "\n",
      "Data shuffled. Epoch:  36\n",
      "Performance on training data: Loss = 0.5420506596565247: Accuracy = 0.6800000071525574\n",
      "Performance on test set: : Loss = 2.9238080978393555: Accuracy = 0.6228630601004008\n",
      "\n",
      "Data shuffled. Epoch:  37\n",
      "Performance on training data: Loss = 0.5881645083427429: Accuracy = 0.6133333444595337\n",
      "Performance on test set: : Loss = 3.0020015239715576: Accuracy = 0.6232701323107782\n",
      "\n",
      "Data shuffled. Epoch:  38\n",
      "Performance on training data: Loss = 0.5310570001602173: Accuracy = 0.7200000286102295\n",
      "Performance on test set: : Loss = 2.96266508102417: Accuracy = 0.6232680417889886\n",
      "\n",
      "Data shuffled. Epoch:  39\n",
      "Performance on training data: Loss = 0.6073540449142456: Accuracy = 0.6600000262260437\n",
      "Performance on test set: : Loss = 2.8226401805877686: Accuracy = 0.6233290588593644\n",
      "\n",
      "Data shuffled. Epoch:  40\n",
      "Performance on training data: Loss = 0.5437855124473572: Accuracy = 0.699999988079071\n",
      "Performance on test set: : Loss = 2.9664862155914307: Accuracy = 0.6231689262710831\n",
      "\n",
      "Data shuffled. Epoch:  41\n",
      "Performance on training data: Loss = 0.5436087250709534: Accuracy = 0.6466666460037231\n",
      "Performance on test set: : Loss = 2.946425437927246: Accuracy = 0.6231038028104672\n",
      "\n",
      "Data shuffled. Epoch:  42\n",
      "Performance on training data: Loss = 0.5816685557365417: Accuracy = 0.6266666650772095\n",
      "Performance on test set: : Loss = 2.879831552505493: Accuracy = 0.6235424463713469\n",
      "\n",
      "Data shuffled. Epoch:  43\n",
      "Performance on training data: Loss = 0.5516614317893982: Accuracy = 0.6600000262260437\n",
      "Performance on test set: : Loss = 3.145408868789673: Accuracy = 0.6237319064454744\n",
      "\n",
      "Data shuffled. Epoch:  44\n",
      "Performance on training data: Loss = 0.5167352557182312: Accuracy = 0.6666666865348816\n",
      "Performance on test set: : Loss = 3.009739398956299: Accuracy = 0.624248346233967\n",
      "\n",
      "Data shuffled. Epoch:  45\n",
      "Performance on training data: Loss = 0.5164361000061035: Accuracy = 0.7133333086967468\n",
      "Performance on test set: : Loss = 3.116058588027954: Accuracy = 0.625099287661828\n",
      "\n",
      "Data shuffled. Epoch:  46\n",
      "Performance on training data: Loss = 0.541757345199585: Accuracy = 0.5933333039283752\n",
      "Performance on test set: : Loss = 3.16719913482666: Accuracy = 0.6256167317335961\n",
      "\n",
      "Data shuffled. Epoch:  47\n",
      "Performance on training data: Loss = 0.5304802656173706: Accuracy = 0.6666666865348816\n",
      "Performance on test set: : Loss = 3.0867838859558105: Accuracy = 0.625774811459358\n",
      "\n",
      "Data shuffled. Epoch:  48\n",
      "Performance on training data: Loss = 0.5145648717880249: Accuracy = 0.6399999856948853\n",
      "Performance on test set: : Loss = 3.0346455574035645: Accuracy = 0.6264378643452095\n",
      "\n",
      "Data shuffled. Epoch:  49\n",
      "Performance on training data: Loss = 0.5432762503623962: Accuracy = 0.6600000262260437\n",
      "Performance on test set: : Loss = 3.2746894359588623: Accuracy = 0.626564066170708\n",
      "\n",
      "Data shuffled. Epoch:  50\n",
      "Performance on training data: Loss = 0.5232017636299133: Accuracy = 0.6466666460037231\n",
      "Performance on test set: : Loss = 3.230208396911621: Accuracy = 0.6266809647035015\n",
      "\n",
      "Data shuffled. Epoch:  51\n",
      "Performance on training data: Loss = 0.5396649837493896: Accuracy = 0.5733333230018616\n",
      "Performance on test set: : Loss = 3.1385552883148193: Accuracy = 0.6269944312255555\n",
      "\n",
      "Data shuffled. Epoch:  52\n",
      "Performance on training data: Loss = 0.5341337323188782: Accuracy = 0.6666666865348816\n",
      "Performance on test set: : Loss = 3.4184255599975586: Accuracy = 0.6271757416095164\n",
      "\n",
      "Data shuffled. Epoch:  53\n",
      "Performance on training data: Loss = 0.6498789191246033: Accuracy = 0.6600000262260437\n",
      "Performance on test set: : Loss = 3.448030710220337: Accuracy = 0.627703751121276\n",
      "\n",
      "Data shuffled. Epoch:  54\n",
      "Performance on training data: Loss = 0.5245208740234375: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 3.4055285453796387: Accuracy = 0.6285298586946666\n",
      "\n",
      "Data shuffled. Epoch:  55\n",
      "Performance on training data: Loss = 0.530042290687561: Accuracy = 0.6266666650772095\n",
      "Performance on test set: : Loss = 3.3069612979888916: Accuracy = 0.6295033378373834\n",
      "\n",
      "Data shuffled. Epoch:  56\n",
      "Performance on training data: Loss = 0.5458179116249084: Accuracy = 0.7200000286102295\n",
      "Performance on test set: : Loss = 3.5111091136932373: Accuracy = 0.6301116206246371\n",
      "\n",
      "Data shuffled. Epoch:  57\n",
      "Performance on training data: Loss = 0.5077325701713562: Accuracy = 0.699999988079071\n",
      "Performance on test set: : Loss = 3.47641658782959: Accuracy = 0.6305233209751376\n",
      "\n",
      "Data shuffled. Epoch:  58\n",
      "Performance on training data: Loss = 0.5148438215255737: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 3.99766206741333: Accuracy = 0.6303497410116283\n",
      "\n",
      "Data shuffled. Epoch:  59\n",
      "Performance on training data: Loss = 0.5211897492408752: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 3.840968608856201: Accuracy = 0.6301178897852313\n",
      "\n",
      "Data shuffled. Epoch:  60\n",
      "Performance on training data: Loss = 0.5223480463027954: Accuracy = 0.6066666841506958\n",
      "Performance on test set: : Loss = 3.5677289962768555: Accuracy = 0.6301418381816967\n",
      "\n",
      "Data shuffled. Epoch:  61\n",
      "Performance on training data: Loss = 0.5396785736083984: Accuracy = 0.6666666865348816\n",
      "Performance on test set: : Loss = 3.7061429023742676: Accuracy = 0.630671885249929\n",
      "\n",
      "Data shuffled. Epoch:  62\n",
      "Performance on training data: Loss = 0.5058457851409912: Accuracy = 0.6733333468437195\n",
      "Performance on test set: : Loss = 4.013097763061523: Accuracy = 0.6306773757540874\n",
      "\n",
      "Data shuffled. Epoch:  63\n",
      "Performance on training data: Loss = 0.5062360763549805: Accuracy = 0.7200000286102295\n",
      "Performance on test set: : Loss = 3.8279504776000977: Accuracy = 0.6309935880551821\n",
      "\n",
      "Data shuffled. Epoch:  64\n",
      "Performance on training data: Loss = 0.485957533121109: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 3.8436825275421143: Accuracy = 0.6316817772673486\n",
      "\n",
      "Data shuffled. Epoch:  65\n",
      "Performance on training data: Loss = 0.5978866219520569: Accuracy = 0.6666666865348816\n",
      "Performance on test set: : Loss = 4.087358474731445: Accuracy = 0.6323250449442355\n",
      "\n",
      "Data shuffled. Epoch:  66\n",
      "Performance on training data: Loss = 0.6272053122520447: Accuracy = 0.6333333253860474\n",
      "Performance on test set: : Loss = 3.8762927055358887: Accuracy = 0.631972096635405\n",
      "\n",
      "Data shuffled. Epoch:  67\n",
      "Performance on training data: Loss = 0.5583781599998474: Accuracy = 0.6133333444595337\n",
      "Performance on test set: : Loss = 3.8710968494415283: Accuracy = 0.6311748423617015\n",
      "\n",
      "Data shuffled. Epoch:  68\n",
      "Performance on training data: Loss = 0.5012534260749817: Accuracy = 0.7200000286102295\n",
      "Performance on test set: : Loss = 4.194153785705566: Accuracy = 0.6300512744150133\n",
      "\n",
      "Data shuffled. Epoch:  69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training data: Loss = 0.5009556412696838: Accuracy = 0.6866666674613953\n",
      "Performance on test set: : Loss = 4.128414630889893: Accuracy = 0.6288767077472174\n",
      "\n",
      "Data shuffled. Epoch:  70\n",
      "Performance on training data: Loss = 0.5040379166603088: Accuracy = 0.699999988079071\n",
      "Performance on test set: : Loss = 4.263802528381348: Accuracy = 0.6278772755539973\n",
      "\n",
      "Data shuffled. Epoch:  71\n",
      "Performance on training data: Loss = 0.5931037664413452: Accuracy = 0.653333306312561\n",
      "Performance on test set: : Loss = 4.042793273925781: Accuracy = 0.6269048395061961\n",
      "\n",
      "Data shuffled. Epoch:  72\n",
      "Performance on training data: Loss = 0.48942291736602783: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 4.206618785858154: Accuracy = 0.626179168217044\n",
      "\n",
      "Data shuffled. Epoch:  73\n",
      "Performance on training data: Loss = 0.5169019103050232: Accuracy = 0.6666666865348816\n",
      "Performance on test set: : Loss = 4.082635879516602: Accuracy = 0.6256477655997268\n",
      "\n",
      "Data shuffled. Epoch:  74\n",
      "Performance on training data: Loss = 0.5021505951881409: Accuracy = 0.6600000262260437\n",
      "Performance on test set: : Loss = 4.055698871612549: Accuracy = 0.6252030325837623\n",
      "\n",
      "Data shuffled. Epoch:  75\n",
      "Performance on training data: Loss = 0.5344014167785645: Accuracy = 0.6333333253860474\n",
      "Performance on test set: : Loss = 4.189474105834961: Accuracy = 0.6245366020475287\n",
      "\n",
      "Data shuffled. Epoch:  76\n",
      "Performance on training data: Loss = 0.5167151093482971: Accuracy = 0.6733333468437195\n",
      "Performance on test set: : Loss = 4.736637592315674: Accuracy = 0.6235385786782778\n",
      "\n",
      "Data shuffled. Epoch:  77\n",
      "Performance on training data: Loss = 0.5347985625267029: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 4.791617393493652: Accuracy = 0.6226647994810518\n",
      "\n",
      "Data shuffled. Epoch:  78\n",
      "Performance on training data: Loss = 0.4978078603744507: Accuracy = 0.7133333086967468\n",
      "Performance on test set: : Loss = 4.607471942901611: Accuracy = 0.6222600728208612\n",
      "\n",
      "Data shuffled. Epoch:  79\n",
      "Performance on training data: Loss = 0.5136945843696594: Accuracy = 0.6600000262260437\n",
      "Performance on test set: : Loss = 4.452760219573975: Accuracy = 0.6223161711189379\n",
      "\n",
      "Data shuffled. Epoch:  80\n",
      "Performance on training data: Loss = 0.5184080004692078: Accuracy = 0.6333333253860474\n",
      "Performance on test set: : Loss = 4.621401786804199: Accuracy = 0.622401546178603\n",
      "\n",
      "Data shuffled. Epoch:  81\n",
      "Performance on training data: Loss = 0.4952991306781769: Accuracy = 0.699999988079071\n",
      "Performance on test set: : Loss = 4.713658332824707: Accuracy = 0.6222713809073745\n",
      "\n",
      "Data shuffled. Epoch:  82\n",
      "Performance on training data: Loss = 0.495125412940979: Accuracy = 0.6933333277702332\n",
      "Performance on test set: : Loss = 4.494003772735596: Accuracy = 0.6216972252411047\n",
      "\n",
      "Data shuffled. Epoch:  83\n",
      "Performance on training data: Loss = 0.48430344462394714: Accuracy = 0.6933333277702332\n",
      "Performance on test set: : Loss = 4.479344367980957: Accuracy = 0.621559755252947\n",
      "\n",
      "Data shuffled. Epoch:  84\n",
      "Performance on training data: Loss = 0.5419893264770508: Accuracy = 0.6800000071525574\n",
      "Performance on test set: : Loss = 4.647441387176514: Accuracy = 0.6212299075828315\n",
      "\n",
      "Data shuffled. Epoch:  85\n",
      "Performance on training data: Loss = 0.506750226020813: Accuracy = 0.6933333277702332\n",
      "Performance on test set: : Loss = 4.273865222930908: Accuracy = 0.6215954107282489\n",
      "\n",
      "Data shuffled. Epoch:  86\n",
      "Performance on training data: Loss = 0.5162420272827148: Accuracy = 0.653333306312561\n",
      "Performance on test set: : Loss = 4.288036346435547: Accuracy = 0.6221842608086687\n",
      "\n",
      "Data shuffled. Epoch:  87\n",
      "Performance on training data: Loss = 0.5078313946723938: Accuracy = 0.6866666674613953\n",
      "Performance on test set: : Loss = 4.508411884307861: Accuracy = 0.6222926410578655\n",
      "\n",
      "Data shuffled. Epoch:  88\n",
      "Performance on training data: Loss = 0.5083858370780945: Accuracy = 0.7133333086967468\n",
      "Performance on test set: : Loss = 4.253571033477783: Accuracy = 0.6222990017968456\n",
      "\n",
      "Data shuffled. Epoch:  89\n",
      "Performance on training data: Loss = 0.5118396282196045: Accuracy = 0.6866666674613953\n",
      "Performance on test set: : Loss = 4.656720161437988: Accuracy = 0.6225418469315419\n",
      "\n",
      "Data shuffled. Epoch:  90\n",
      "Performance on training data: Loss = 0.4882213771343231: Accuracy = 0.7333333492279053\n",
      "Performance on test set: : Loss = 4.430715084075928: Accuracy = 0.6225028983324319\n",
      "\n",
      "Data shuffled. Epoch:  91\n",
      "Performance on training data: Loss = 0.5087839365005493: Accuracy = 0.6933333277702332\n",
      "Performance on test set: : Loss = 4.181721210479736: Accuracy = 0.6230228234689977\n",
      "\n",
      "Data shuffled. Epoch:  92\n",
      "Performance on training data: Loss = 0.49076607823371887: Accuracy = 0.6866666674613953\n",
      "Performance on test set: : Loss = 4.304341793060303: Accuracy = 0.6233801852356669\n",
      "\n",
      "Data shuffled. Epoch:  93\n",
      "Performance on training data: Loss = 0.4922161400318146: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 4.418283939361572: Accuracy = 0.6237462575827414\n",
      "\n",
      "Data shuffled. Epoch:  94\n",
      "Performance on training data: Loss = 0.5337546467781067: Accuracy = 0.6733333468437195\n",
      "Performance on test set: : Loss = 4.957889080047607: Accuracy = 0.6238973993594223\n",
      "\n",
      "Data shuffled. Epoch:  95\n",
      "Performance on training data: Loss = 0.9544790387153625: Accuracy = 0.6000000238418579\n",
      "Performance on test set: : Loss = 3.997716188430786: Accuracy = 0.6238081060749042\n",
      "\n",
      "Data shuffled. Epoch:  96\n",
      "Performance on training data: Loss = 0.5701337456703186: Accuracy = 0.6866666674613953\n",
      "Performance on test set: : Loss = 3.401071071624756: Accuracy = 0.6239598986741224\n",
      "\n",
      "Data shuffled. Epoch:  97\n",
      "Performance on training data: Loss = 0.539000928401947: Accuracy = 0.6800000071525574\n",
      "Performance on test set: : Loss = 3.3749053478240967: Accuracy = 0.6240879476416179\n",
      "\n",
      "Data shuffled. Epoch:  98\n",
      "Performance on training data: Loss = 0.49961724877357483: Accuracy = 0.6933333277702332\n",
      "Performance on test set: : Loss = 3.2566142082214355: Accuracy = 0.6243271035325606\n",
      "\n",
      "Data shuffled. Epoch:  99\n",
      "Performance on training data: Loss = 0.5136968493461609: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 3.5537781715393066: Accuracy = 0.6246242968397786\n",
      "\n",
      "Data shuffled. Epoch:  100\n",
      "Performance on training data: Loss = 0.4848856031894684: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 3.690840005874634: Accuracy = 0.6248418647370878\n",
      "\n",
      "Data shuffled. Epoch:  101\n",
      "Performance on training data: Loss = 0.505466639995575: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 3.9103972911834717: Accuracy = 0.6250660829693123\n",
      "\n",
      "Data shuffled. Epoch:  102\n",
      "Performance on training data: Loss = 0.506697952747345: Accuracy = 0.6800000071525574\n",
      "Performance on test set: : Loss = 4.11453914642334: Accuracy = 0.6253206818398436\n",
      "\n",
      "Data shuffled. Epoch:  103\n",
      "Performance on training data: Loss = 0.4966667592525482: Accuracy = 0.7200000286102295\n",
      "Performance on test set: : Loss = 4.220263957977295: Accuracy = 0.6255249007597291\n",
      "\n",
      "Data shuffled. Epoch:  104\n",
      "Performance on training data: Loss = 0.4841712415218353: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 4.236649513244629: Accuracy = 0.6257122392464799\n",
      "\n",
      "Data shuffled. Epoch:  105\n",
      "Performance on training data: Loss = 0.49932003021240234: Accuracy = 0.6666666865348816\n",
      "Performance on test set: : Loss = 4.444331645965576: Accuracy = 0.6259065823194402\n",
      "\n",
      "Data shuffled. Epoch:  106\n",
      "Performance on training data: Loss = 0.49582383036613464: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 4.493887424468994: Accuracy = 0.6260049482823281\n",
      "\n",
      "Data shuffled. Epoch:  107\n",
      "Performance on training data: Loss = 0.4831162393093109: Accuracy = 0.7266666889190674\n",
      "Performance on test set: : Loss = 4.58624792098999: Accuracy = 0.6261221895456198\n",
      "\n",
      "Data shuffled. Epoch:  108\n",
      "Performance on training data: Loss = 0.5075368285179138: Accuracy = 0.7200000286102295\n",
      "Performance on test set: : Loss = 4.672289848327637: Accuracy = 0.6261961024053116\n",
      "\n",
      "Data shuffled. Epoch:  109\n",
      "Performance on training data: Loss = 0.5389829277992249: Accuracy = 0.6733333468437195\n",
      "Performance on test set: : Loss = 4.775864124298096: Accuracy = 0.6263582297815782\n",
      "\n",
      "Data shuffled. Epoch:  110\n",
      "Performance on training data: Loss = 0.5338265299797058: Accuracy = 0.6800000071525574\n",
      "Performance on test set: : Loss = 4.90477991104126: Accuracy = 0.6264487983710112\n",
      "\n",
      "Data shuffled. Epoch:  111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training data: Loss = 0.5092282891273499: Accuracy = 0.6933333277702332\n",
      "Performance on test set: : Loss = 4.865481376647949: Accuracy = 0.6265496619220414\n",
      "\n",
      "Data shuffled. Epoch:  112\n",
      "Performance on training data: Loss = 0.4984813928604126: Accuracy = 0.7666666507720947\n",
      "Performance on test set: : Loss = 4.727122783660889: Accuracy = 0.6267259291959074\n",
      "\n",
      "Data shuffled. Epoch:  113\n",
      "Performance on training data: Loss = 0.5187425017356873: Accuracy = 0.6866666674613953\n",
      "Performance on test set: : Loss = 4.946290969848633: Accuracy = 0.6268774339558133\n",
      "\n",
      "Data shuffled. Epoch:  114\n",
      "Performance on training data: Loss = 0.47231388092041016: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 4.956661224365234: Accuracy = 0.6270048195645849\n",
      "\n",
      "Data shuffled. Epoch:  115\n",
      "Performance on training data: Loss = 0.5012576580047607: Accuracy = 0.7133333086967468\n",
      "Performance on test set: : Loss = 4.984684467315674: Accuracy = 0.6270934793723345\n",
      "\n",
      "Data shuffled. Epoch:  116\n",
      "Performance on training data: Loss = 0.49591323733329773: Accuracy = 0.7333333492279053\n",
      "Performance on test set: : Loss = 5.050983428955078: Accuracy = 0.6271824858634251\n",
      "\n",
      "Data shuffled. Epoch:  117\n",
      "Performance on training data: Loss = 0.48870614171028137: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.041078567504883: Accuracy = 0.6272282963608407\n",
      "\n",
      "Data shuffled. Epoch:  118\n",
      "Performance on training data: Loss = 0.46541300415992737: Accuracy = 0.7333333492279053\n",
      "Performance on test set: : Loss = 5.07643985748291: Accuracy = 0.6273033597017227\n",
      "\n",
      "Data shuffled. Epoch:  119\n",
      "Performance on training data: Loss = 0.46303263306617737: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 5.150439739227295: Accuracy = 0.6273883018256795\n",
      "\n",
      "Data shuffled. Epoch:  120\n",
      "Performance on training data: Loss = 0.4706178903579712: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 5.233796119689941: Accuracy = 0.6274718594989166\n",
      "\n",
      "Data shuffled. Epoch:  121\n",
      "Performance on training data: Loss = 0.49333545565605164: Accuracy = 0.7266666889190674\n",
      "Performance on test set: : Loss = 5.391716480255127: Accuracy = 0.6276236352638342\n",
      "\n",
      "Data shuffled. Epoch:  122\n",
      "Performance on training data: Loss = 0.49664515256881714: Accuracy = 0.6866666674613953\n",
      "Performance on test set: : Loss = 5.465710639953613: Accuracy = 0.6277163606292562\n",
      "\n",
      "Data shuffled. Epoch:  123\n",
      "Performance on training data: Loss = 0.4808492064476013: Accuracy = 0.7333333492279053\n",
      "Performance on test set: : Loss = 5.251377105712891: Accuracy = 0.627715933302223\n",
      "\n",
      "Data shuffled. Epoch:  124\n",
      "Performance on training data: Loss = 0.49951791763305664: Accuracy = 0.7266666889190674\n",
      "Performance on test set: : Loss = 5.4040117263793945: Accuracy = 0.6276957967963915\n",
      "\n",
      "Data shuffled. Epoch:  125\n",
      "Performance on training data: Loss = 0.4912790358066559: Accuracy = 0.7400000095367432\n",
      "Performance on test set: : Loss = 5.366008758544922: Accuracy = 0.6277295683992451\n",
      "\n",
      "Data shuffled. Epoch:  126\n",
      "Performance on training data: Loss = 0.498246967792511: Accuracy = 0.699999988079071\n",
      "Performance on test set: : Loss = 5.543319225311279: Accuracy = 0.6278047302253794\n",
      "\n",
      "Data shuffled. Epoch:  127\n",
      "Performance on training data: Loss = 0.49055933952331543: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.467857360839844: Accuracy = 0.6278019567034436\n",
      "\n",
      "Data shuffled. Epoch:  128\n",
      "Performance on training data: Loss = 0.5101765990257263: Accuracy = 0.6933333277702332\n",
      "Performance on test set: : Loss = 5.498794078826904: Accuracy = 0.627882399352788\n",
      "\n",
      "Data shuffled. Epoch:  129\n",
      "Performance on training data: Loss = 0.46039459109306335: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 5.550770282745361: Accuracy = 0.6279908787191386\n",
      "\n",
      "Data shuffled. Epoch:  130\n",
      "Performance on training data: Loss = 0.47726303339004517: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.596493721008301: Accuracy = 0.6280413424330211\n",
      "\n",
      "Data shuffled. Epoch:  131\n",
      "Performance on training data: Loss = 0.4642884433269501: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 5.480219841003418: Accuracy = 0.6280236408413689\n",
      "\n",
      "Data shuffled. Epoch:  132\n",
      "Performance on training data: Loss = 0.4672146141529083: Accuracy = 0.7400000095367432\n",
      "Performance on test set: : Loss = 5.577066421508789: Accuracy = 0.6280747057376509\n",
      "\n",
      "Data shuffled. Epoch:  133\n",
      "Performance on training data: Loss = 0.4433187246322632: Accuracy = 0.7400000095367432\n",
      "Performance on test set: : Loss = 5.714039325714111: Accuracy = 0.6280501976133847\n",
      "\n",
      "Data shuffled. Epoch:  134\n",
      "Performance on training data: Loss = 0.4564318358898163: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.707543849945068: Accuracy = 0.628069193900144\n",
      "\n",
      "Data shuffled. Epoch:  135\n",
      "Performance on training data: Loss = 0.48576903343200684: Accuracy = 0.7400000095367432\n",
      "Performance on test set: : Loss = 6.128355503082275: Accuracy = 0.6281089794989501\n",
      "\n",
      "Data shuffled. Epoch:  136\n",
      "Performance on training data: Loss = 0.4773513376712799: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.696094989776611: Accuracy = 0.6281205425540028\n",
      "\n",
      "Data shuffled. Epoch:  137\n",
      "Performance on training data: Loss = 0.44773074984550476: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 5.671365737915039: Accuracy = 0.6282194378016395\n",
      "\n",
      "Data shuffled. Epoch:  138\n",
      "Performance on training data: Loss = 0.4829605221748352: Accuracy = 0.7333333492279053\n",
      "Performance on test set: : Loss = 5.63601016998291: Accuracy = 0.6282794671244568\n",
      "\n",
      "Data shuffled. Epoch:  139\n",
      "Performance on training data: Loss = 0.46276235580444336: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 5.742982387542725: Accuracy = 0.6283465304708612\n",
      "\n",
      "Data shuffled. Epoch:  140\n",
      "Performance on training data: Loss = 0.4850062131881714: Accuracy = 0.7133333086967468\n",
      "Performance on test set: : Loss = 5.489208698272705: Accuracy = 0.6284177103046311\n",
      "\n",
      "Data shuffled. Epoch:  141\n",
      "Performance on training data: Loss = 0.4705146849155426: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.751163005828857: Accuracy = 0.62848792507974\n",
      "\n",
      "Data shuffled. Epoch:  142\n",
      "Performance on training data: Loss = 0.46626153588294983: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.657692909240723: Accuracy = 0.6285356610157469\n",
      "\n",
      "Data shuffled. Epoch:  143\n",
      "Performance on training data: Loss = 0.4720441699028015: Accuracy = 0.7133333086967468\n",
      "Performance on test set: : Loss = 5.541317462921143: Accuracy = 0.6285622316382049\n",
      "\n",
      "Data shuffled. Epoch:  144\n",
      "Performance on training data: Loss = 0.4612676501274109: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 5.428810119628906: Accuracy = 0.6286073561061408\n",
      "\n",
      "Data shuffled. Epoch:  145\n",
      "Performance on training data: Loss = 0.4423990249633789: Accuracy = 0.7933333516120911\n",
      "Performance on test set: : Loss = 5.5758280754089355: Accuracy = 0.6287079678765852\n",
      "\n",
      "Data shuffled. Epoch:  146\n",
      "Performance on training data: Loss = 0.447236567735672: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 5.673888206481934: Accuracy = 0.6287508873654996\n",
      "\n",
      "Data shuffled. Epoch:  147\n",
      "Performance on training data: Loss = 0.43930700421333313: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 5.781704902648926: Accuracy = 0.6288414115217312\n",
      "\n",
      "Data shuffled. Epoch:  148\n",
      "Performance on training data: Loss = 0.486878901720047: Accuracy = 0.7400000095367432\n",
      "Performance on test set: : Loss = 5.93734884262085: Accuracy = 0.6288663628412803\n",
      "\n",
      "Data shuffled. Epoch:  149\n",
      "Performance on training data: Loss = 0.4597782492637634: Accuracy = 0.7666666507720947\n",
      "Performance on test set: : Loss = 5.083366394042969: Accuracy = 0.6290053205213226\n",
      "\n",
      "Data shuffled. Epoch:  150\n",
      "Performance on training data: Loss = 0.45687198638916016: Accuracy = 0.7733333110809326\n",
      "Performance on test set: : Loss = 5.176383018493652: Accuracy = 0.629182359684535\n",
      "\n",
      "Data shuffled. Epoch:  151\n",
      "Performance on training data: Loss = 0.4397164285182953: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 5.642218589782715: Accuracy = 0.6292562939962045\n",
      "\n",
      "Data shuffled. Epoch:  152\n",
      "Performance on training data: Loss = 0.43691757321357727: Accuracy = 0.8066666722297668\n",
      "Performance on test set: : Loss = 5.20654821395874: Accuracy = 0.6293245456327975\n",
      "\n",
      "Data shuffled. Epoch:  153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training data: Loss = 0.41304612159729004: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 5.208054065704346: Accuracy = 0.6295488495951497\n",
      "\n",
      "Data shuffled. Epoch:  154\n",
      "Performance on training data: Loss = 0.4871339797973633: Accuracy = 0.7200000286102295\n",
      "Performance on test set: : Loss = 5.652740478515625: Accuracy = 0.6298708305001733\n",
      "\n",
      "Data shuffled. Epoch:  155\n",
      "Performance on training data: Loss = 0.4492270350456238: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 5.114860534667969: Accuracy = 0.6300545239867514\n",
      "\n",
      "Data shuffled. Epoch:  156\n",
      "Performance on training data: Loss = 0.45619913935661316: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 5.802594184875488: Accuracy = 0.6303089294449933\n",
      "\n",
      "Data shuffled. Epoch:  157\n",
      "Performance on training data: Loss = 0.45046162605285645: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 5.255133628845215: Accuracy = 0.6306064197844847\n",
      "\n",
      "Data shuffled. Epoch:  158\n",
      "Performance on training data: Loss = 0.4379257261753082: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 5.540700435638428: Accuracy = 0.630899888993593\n",
      "\n",
      "Data shuffled. Epoch:  159\n",
      "Performance on training data: Loss = 0.4381435513496399: Accuracy = 0.7933333516120911\n",
      "Performance on test set: : Loss = 5.552480220794678: Accuracy = 0.6312528326659917\n",
      "\n",
      "Data shuffled. Epoch:  160\n",
      "Performance on training data: Loss = 0.47116342186927795: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.497354984283447: Accuracy = 0.631615039131806\n",
      "\n",
      "Data shuffled. Epoch:  161\n",
      "Performance on training data: Loss = 0.42093563079833984: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 5.202107906341553: Accuracy = 0.631925019663923\n",
      "\n",
      "Data shuffled. Epoch:  162\n",
      "Performance on training data: Loss = 0.4600273668766022: Accuracy = 0.746666669845581\n",
      "Performance on test set: : Loss = 5.261405944824219: Accuracy = 0.6322068452665841\n",
      "\n",
      "Data shuffled. Epoch:  163\n",
      "Performance on training data: Loss = 0.448775976896286: Accuracy = 0.7400000095367432\n",
      "Performance on test set: : Loss = 6.019932746887207: Accuracy = 0.6324142413027276\n",
      "\n",
      "Data shuffled. Epoch:  164\n",
      "Performance on training data: Loss = 0.45868274569511414: Accuracy = 0.7666666507720947\n",
      "Performance on test set: : Loss = 6.000491142272949: Accuracy = 0.6324670593811108\n",
      "\n",
      "Data shuffled. Epoch:  165\n",
      "Performance on training data: Loss = 0.43845951557159424: Accuracy = 0.7733333110809326\n",
      "Performance on test set: : Loss = 6.121782302856445: Accuracy = 0.632557421403718\n",
      "\n",
      "Data shuffled. Epoch:  166\n",
      "Performance on training data: Loss = 0.48268818855285645: Accuracy = 0.7666666507720947\n",
      "Performance on test set: : Loss = 5.140632152557373: Accuracy = 0.6327443893875683\n",
      "\n",
      "Data shuffled. Epoch:  167\n",
      "Performance on training data: Loss = 0.43997201323509216: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 5.432270526885986: Accuracy = 0.6329799829907357\n",
      "\n",
      "Data shuffled. Epoch:  168\n",
      "Performance on training data: Loss = 0.49186259508132935: Accuracy = 0.7333333492279053\n",
      "Performance on test set: : Loss = 5.463848114013672: Accuracy = 0.6331936811732094\n",
      "\n",
      "Data shuffled. Epoch:  169\n",
      "Performance on training data: Loss = 0.44753432273864746: Accuracy = 0.7733333110809326\n",
      "Performance on test set: : Loss = 5.409151077270508: Accuracy = 0.6333741106089222\n",
      "\n",
      "Data shuffled. Epoch:  170\n",
      "Performance on training data: Loss = 0.4092804789543152: Accuracy = 0.8199999928474426\n",
      "Performance on test set: : Loss = 5.630518913269043: Accuracy = 0.6335827361716079\n",
      "\n",
      "Data shuffled. Epoch:  171\n",
      "Performance on training data: Loss = 0.48061954975128174: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 5.270815849304199: Accuracy = 0.633792809715307\n",
      "\n",
      "Data shuffled. Epoch:  172\n",
      "Performance on training data: Loss = 0.4501252770423889: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 6.433844089508057: Accuracy = 0.6338940297216122\n",
      "\n",
      "Data shuffled. Epoch:  173\n",
      "Performance on training data: Loss = 0.41894060373306274: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 5.501063823699951: Accuracy = 0.6339930598531491\n",
      "\n",
      "Data shuffled. Epoch:  174\n",
      "Performance on training data: Loss = 0.45980164408683777: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 6.268275737762451: Accuracy = 0.634016681208971\n",
      "\n",
      "Data shuffled. Epoch:  175\n",
      "Performance on training data: Loss = 0.4333203136920929: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 6.385827541351318: Accuracy = 0.634083353834137\n",
      "\n",
      "Data shuffled. Epoch:  176\n",
      "Performance on training data: Loss = 0.3881402015686035: Accuracy = 0.8199999928474426\n",
      "Performance on test set: : Loss = 6.273502826690674: Accuracy = 0.6340572929748566\n",
      "\n",
      "Data shuffled. Epoch:  177\n",
      "Performance on training data: Loss = 0.4112154245376587: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 6.66391134262085: Accuracy = 0.634098675897546\n",
      "\n",
      "Data shuffled. Epoch:  178\n",
      "Performance on training data: Loss = 0.4363066554069519: Accuracy = 0.753333330154419\n",
      "Performance on test set: : Loss = 6.651495933532715: Accuracy = 0.6340893133644263\n",
      "\n",
      "Data shuffled. Epoch:  179\n",
      "Performance on training data: Loss = 0.3876783549785614: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 6.50432014465332: Accuracy = 0.6341535220053724\n",
      "\n",
      "Data shuffled. Epoch:  180\n",
      "Performance on training data: Loss = 0.41924628615379333: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 6.246058464050293: Accuracy = 0.6342170393842622\n",
      "\n",
      "Data shuffled. Epoch:  181\n",
      "Performance on training data: Loss = 0.5021544098854065: Accuracy = 0.7066666483879089\n",
      "Performance on test set: : Loss = 6.243877410888672: Accuracy = 0.6342452199174023\n",
      "\n",
      "Data shuffled. Epoch:  182\n",
      "Performance on training data: Loss = 0.43282151222229004: Accuracy = 0.8199999928474426\n",
      "Performance on test set: : Loss = 6.475057125091553: Accuracy = 0.6343532534984387\n",
      "\n",
      "Data shuffled. Epoch:  183\n",
      "Performance on training data: Loss = 0.41679590940475464: Accuracy = 0.8133333325386047\n",
      "Performance on test set: : Loss = 6.510499477386475: Accuracy = 0.6344742472091626\n",
      "\n",
      "Data shuffled. Epoch:  184\n",
      "Performance on training data: Loss = 0.3888919949531555: Accuracy = 0.8199999928474426\n",
      "Performance on test set: : Loss = 6.569066524505615: Accuracy = 0.6345648783253632\n",
      "\n",
      "Data shuffled. Epoch:  185\n",
      "Performance on training data: Loss = 0.4169074594974518: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 6.138924598693848: Accuracy = 0.6347177747661048\n",
      "\n",
      "Data shuffled. Epoch:  186\n",
      "Performance on training data: Loss = 0.39218470454216003: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 6.1667962074279785: Accuracy = 0.6347819872887809\n",
      "\n",
      "Data shuffled. Epoch:  187\n",
      "Performance on training data: Loss = 0.48110243678092957: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 6.459970951080322: Accuracy = 0.6348859446452941\n",
      "\n",
      "Data shuffled. Epoch:  188\n",
      "Performance on training data: Loss = 0.4236239790916443: Accuracy = 0.8066666722297668\n",
      "Performance on test set: : Loss = 5.503400802612305: Accuracy = 0.634929892011346\n",
      "\n",
      "Data shuffled. Epoch:  189\n",
      "Performance on training data: Loss = 0.46745920181274414: Accuracy = 0.7666666507720947\n",
      "Performance on test set: : Loss = 6.344488620758057: Accuracy = 0.6350214819346215\n",
      "\n",
      "Data shuffled. Epoch:  190\n",
      "Performance on training data: Loss = 0.3945181369781494: Accuracy = 0.8199999928474426\n",
      "Performance on test set: : Loss = 5.7124762535095215: Accuracy = 0.6352488064380551\n",
      "\n",
      "Data shuffled. Epoch:  191\n",
      "Performance on training data: Loss = 0.42360782623291016: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 6.134347915649414: Accuracy = 0.6355080205108576\n",
      "\n",
      "Data shuffled. Epoch:  192\n",
      "Performance on training data: Loss = 0.4336082935333252: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 5.821301460266113: Accuracy = 0.6357392783314076\n",
      "\n",
      "Data shuffled. Epoch:  193\n",
      "Performance on training data: Loss = 0.3999062478542328: Accuracy = 0.7733333110809326\n",
      "Performance on test set: : Loss = 5.577056407928467: Accuracy = 0.6359363918193397\n",
      "\n",
      "Data shuffled. Epoch:  194\n",
      "Performance on training data: Loss = 0.4074137806892395: Accuracy = 0.7733333110809326\n",
      "Performance on test set: : Loss = 6.050844192504883: Accuracy = 0.6361355987718573\n",
      "\n",
      "Data shuffled. Epoch:  195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training data: Loss = 0.4274848401546478: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 6.083785057067871: Accuracy = 0.6362399888269951\n",
      "\n",
      "Data shuffled. Epoch:  196\n",
      "Performance on training data: Loss = 0.38527926802635193: Accuracy = 0.8266666531562805\n",
      "Performance on test set: : Loss = 5.803347587585449: Accuracy = 0.6364581575634676\n",
      "\n",
      "Data shuffled. Epoch:  197\n",
      "Performance on training data: Loss = 0.4595560133457184: Accuracy = 0.8066666722297668\n",
      "Performance on test set: : Loss = 5.627978801727295: Accuracy = 0.6366788168278301\n",
      "\n",
      "Data shuffled. Epoch:  198\n",
      "Performance on training data: Loss = 0.4547613561153412: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 6.5911865234375: Accuracy = 0.6367714627632481\n",
      "\n",
      "Data shuffled. Epoch:  199\n",
      "Performance on training data: Loss = 0.4118472933769226: Accuracy = 0.8333333134651184\n",
      "Performance on test set: : Loss = 6.3833327293396: Accuracy = 0.6368142445197025\n",
      "\n",
      "Data shuffled. Epoch:  200\n",
      "Performance on training data: Loss = 0.5021853446960449: Accuracy = 0.7266666889190674\n",
      "Performance on test set: : Loss = 6.584883689880371: Accuracy = 0.6368893332501322\n",
      "\n",
      "Data shuffled. Epoch:  201\n",
      "Performance on training data: Loss = 0.3957545757293701: Accuracy = 0.8133333325386047\n",
      "Performance on test set: : Loss = 6.806643962860107: Accuracy = 0.6369446818869239\n",
      "\n",
      "Data shuffled. Epoch:  202\n",
      "Performance on training data: Loss = 0.42398518323898315: Accuracy = 0.8066666722297668\n",
      "Performance on test set: : Loss = 6.640432357788086: Accuracy = 0.6369954268673688\n",
      "\n",
      "Data shuffled. Epoch:  203\n",
      "Performance on training data: Loss = 0.4137348532676697: Accuracy = 0.7933333516120911\n",
      "Performance on test set: : Loss = 6.8247857093811035: Accuracy = 0.6370370011159106\n",
      "\n",
      "Data shuffled. Epoch:  204\n",
      "Performance on training data: Loss = 0.3973236083984375: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 6.789768695831299: Accuracy = 0.6370657310400484\n",
      "\n",
      "Data shuffled. Epoch:  205\n",
      "Performance on training data: Loss = 0.354500412940979: Accuracy = 0.8199999928474426\n",
      "Performance on test set: : Loss = 6.717919826507568: Accuracy = 0.63713376726262\n",
      "\n",
      "Data shuffled. Epoch:  206\n",
      "Performance on training data: Loss = 0.39132198691368103: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 6.522067546844482: Accuracy = 0.6372097257874376\n",
      "\n",
      "Data shuffled. Epoch:  207\n",
      "Performance on training data: Loss = 0.4338926076889038: Accuracy = 0.8266666531562805\n",
      "Performance on test set: : Loss = 7.062013149261475: Accuracy = 0.6372764063547348\n",
      "\n",
      "Data shuffled. Epoch:  208\n",
      "Performance on training data: Loss = 0.4041697084903717: Accuracy = 0.8133333325386047\n",
      "Performance on test set: : Loss = 6.784057140350342: Accuracy = 0.6373141178168346\n",
      "\n",
      "Data shuffled. Epoch:  209\n",
      "Performance on training data: Loss = 0.4070669114589691: Accuracy = 0.8066666722297668\n",
      "Performance on test set: : Loss = 6.9891438484191895: Accuracy = 0.6373460728953873\n",
      "\n",
      "Data shuffled. Epoch:  210\n",
      "Performance on training data: Loss = 0.4088030457496643: Accuracy = 0.7933333516120911\n",
      "Performance on test set: : Loss = 6.865337371826172: Accuracy = 0.6373716441223178\n",
      "\n",
      "Data shuffled. Epoch:  211\n",
      "Performance on training data: Loss = 0.3928874731063843: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 6.713385105133057: Accuracy = 0.6374407753010822\n",
      "\n",
      "Data shuffled. Epoch:  212\n",
      "Performance on training data: Loss = 0.35714617371559143: Accuracy = 0.8666666746139526\n",
      "Performance on test set: : Loss = 6.530955791473389: Accuracy = 0.6374949204609426\n",
      "\n",
      "Data shuffled. Epoch:  213\n",
      "Performance on training data: Loss = 0.3856877088546753: Accuracy = 0.8399999737739563\n",
      "Performance on test set: : Loss = 7.095248222351074: Accuracy = 0.6375425527284772\n",
      "\n",
      "Data shuffled. Epoch:  214\n",
      "Performance on training data: Loss = 0.37215903401374817: Accuracy = 0.8399999737739563\n",
      "Performance on test set: : Loss = 6.848551273345947: Accuracy = 0.6375503149607034\n",
      "\n",
      "Data shuffled. Epoch:  215\n",
      "Performance on training data: Loss = 0.41467881202697754: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 6.819243907928467: Accuracy = 0.6375899495119689\n",
      "\n",
      "Data shuffled. Epoch:  216\n",
      "Performance on training data: Loss = 0.4008980691432953: Accuracy = 0.8066666722297668\n",
      "Performance on test set: : Loss = 7.012238025665283: Accuracy = 0.637631294207963\n",
      "\n",
      "Data shuffled. Epoch:  217\n",
      "Performance on training data: Loss = 0.42858174443244934: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 7.057840347290039: Accuracy = 0.6376766960858075\n",
      "\n",
      "Data shuffled. Epoch:  218\n",
      "Performance on training data: Loss = 0.43495601415634155: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 6.732884407043457: Accuracy = 0.6377107648109119\n",
      "\n",
      "Data shuffled. Epoch:  219\n",
      "Performance on training data: Loss = 0.4505687355995178: Accuracy = 0.7933333516120911\n",
      "Performance on test set: : Loss = 6.604803562164307: Accuracy = 0.6378258342380371\n",
      "\n",
      "Data shuffled. Epoch:  220\n",
      "Performance on training data: Loss = 0.44428467750549316: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 5.613512992858887: Accuracy = 0.6379831563016888\n",
      "\n",
      "Data shuffled. Epoch:  221\n",
      "Performance on training data: Loss = 0.4096505343914032: Accuracy = 0.7666666507720947\n",
      "Performance on test set: : Loss = 5.533616542816162: Accuracy = 0.6381184658930852\n",
      "\n",
      "Data shuffled. Epoch:  222\n",
      "Performance on training data: Loss = 0.45191049575805664: Accuracy = 0.7733333110809326\n",
      "Performance on test set: : Loss = 5.626540660858154: Accuracy = 0.6382924764943106\n",
      "\n",
      "Data shuffled. Epoch:  223\n",
      "Performance on training data: Loss = 0.42233821749687195: Accuracy = 0.8066666722297668\n",
      "Performance on test set: : Loss = 5.568920612335205: Accuracy = 0.6384725794540854\n",
      "\n",
      "Data shuffled. Epoch:  224\n",
      "Performance on training data: Loss = 0.44677531719207764: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 5.894733428955078: Accuracy = 0.6385971516260337\n",
      "\n",
      "Data shuffled. Epoch:  225\n",
      "Performance on training data: Loss = 0.44888249039649963: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 5.646336078643799: Accuracy = 0.6387380683357599\n",
      "\n",
      "Data shuffled. Epoch:  226\n",
      "Performance on training data: Loss = 0.43402883410453796: Accuracy = 0.7733333110809326\n",
      "Performance on test set: : Loss = 6.119024753570557: Accuracy = 0.6388589657791875\n",
      "\n",
      "Data shuffled. Epoch:  227\n",
      "Performance on training data: Loss = 0.42510753870010376: Accuracy = 0.7666666507720947\n",
      "Performance on test set: : Loss = 6.731706619262695: Accuracy = 0.6389388070138624\n",
      "\n",
      "Data shuffled. Epoch:  228\n",
      "Performance on training data: Loss = 0.39851436018943787: Accuracy = 0.8333333134651184\n",
      "Performance on test set: : Loss = 5.880383491516113: Accuracy = 0.6390700799150073\n",
      "\n",
      "Data shuffled. Epoch:  229\n",
      "Performance on training data: Loss = 0.40477150678634644: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 5.703988075256348: Accuracy = 0.6392404786720208\n",
      "\n",
      "Data shuffled. Epoch:  230\n",
      "Performance on training data: Loss = 0.4133811593055725: Accuracy = 0.7866666913032532\n",
      "Performance on test set: : Loss = 5.559953212738037: Accuracy = 0.6395030325200577\n",
      "\n",
      "Data shuffled. Epoch:  231\n",
      "Performance on training data: Loss = 0.3873264491558075: Accuracy = 0.8066666722297668\n",
      "Performance on test set: : Loss = 5.65602970123291: Accuracy = 0.6397935796116946\n",
      "\n",
      "Data shuffled. Epoch:  232\n",
      "Performance on training data: Loss = 0.3677152991294861: Accuracy = 0.8266666531562805\n",
      "Performance on test set: : Loss = 5.7429680824279785: Accuracy = 0.6400421865340205\n",
      "\n",
      "Data shuffled. Epoch:  233\n",
      "Performance on training data: Loss = 0.3566281795501709: Accuracy = 0.8333333134651184\n",
      "Performance on test set: : Loss = 5.5332560539245605: Accuracy = 0.6402016202335518\n",
      "\n",
      "Data shuffled. Epoch:  234\n",
      "Performance on training data: Loss = 0.42881256341934204: Accuracy = 0.7733333110809326\n",
      "Performance on test set: : Loss = 6.034660339355469: Accuracy = 0.6403501345957486\n",
      "\n",
      "Data shuffled. Epoch:  235\n",
      "Performance on training data: Loss = 0.3913672864437103: Accuracy = 0.8399999737739563\n",
      "Performance on test set: : Loss = 5.842506408691406: Accuracy = 0.6405329942337543\n",
      "\n",
      "Data shuffled. Epoch:  236\n",
      "Performance on training data: Loss = 0.39414575695991516: Accuracy = 0.8266666531562805\n",
      "Performance on test set: : Loss = 5.810131072998047: Accuracy = 0.6407048110754701\n",
      "\n",
      "Data shuffled. Epoch:  237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training data: Loss = 0.3730604350566864: Accuracy = 0.7933333516120911\n",
      "Performance on test set: : Loss = 5.728892803192139: Accuracy = 0.6409415586433078\n",
      "\n",
      "Data shuffled. Epoch:  238\n",
      "Performance on training data: Loss = 0.36782434582710266: Accuracy = 0.8333333134651184\n",
      "Performance on test set: : Loss = 6.002163410186768: Accuracy = 0.6411368240961268\n",
      "\n",
      "Data shuffled. Epoch:  239\n",
      "Performance on training data: Loss = 0.4007411003112793: Accuracy = 0.800000011920929\n",
      "Performance on test set: : Loss = 5.921393394470215: Accuracy = 0.6413590013820258\n",
      "\n",
      "Data shuffled. Epoch:  240\n",
      "Performance on training data: Loss = 0.4995484948158264: Accuracy = 0.7266666889190674\n",
      "Performance on test set: : Loss = 5.628505229949951: Accuracy = 0.6415437296027705\n",
      "\n",
      "Data shuffled. Epoch:  241\n",
      "Performance on training data: Loss = 0.45008987188339233: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 5.979395866394043: Accuracy = 0.6417241731992259\n",
      "\n",
      "Data shuffled. Epoch:  242\n",
      "Performance on training data: Loss = 0.38688257336616516: Accuracy = 0.8199999928474426\n",
      "Performance on test set: : Loss = 5.6816816329956055: Accuracy = 0.6418505185458948\n",
      "\n",
      "Data shuffled. Epoch:  243\n",
      "Performance on training data: Loss = 0.43441304564476013: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 5.652767658233643: Accuracy = 0.6419905537962434\n",
      "\n",
      "Data shuffled. Epoch:  244\n",
      "Performance on training data: Loss = 0.4215336740016937: Accuracy = 0.7599999904632568\n",
      "Performance on test set: : Loss = 5.677155017852783: Accuracy = 0.6421575012880459\n",
      "\n",
      "Data shuffled. Epoch:  245\n",
      "Performance on training data: Loss = 0.4114210307598114: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 6.759422302246094: Accuracy = 0.6423220959916178\n",
      "\n",
      "Data shuffled. Epoch:  246\n",
      "Performance on training data: Loss = 0.4609733521938324: Accuracy = 0.8266666531562805\n",
      "Performance on test set: : Loss = 5.803977012634277: Accuracy = 0.6425310403167407\n",
      "\n",
      "Data shuffled. Epoch:  247\n",
      "Performance on training data: Loss = 0.4380825161933899: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 5.638791561126709: Accuracy = 0.64274469937126\n",
      "\n",
      "Data shuffled. Epoch:  248\n",
      "Performance on training data: Loss = 0.4028656482696533: Accuracy = 0.7799999713897705\n",
      "Performance on test set: : Loss = 5.750577926635742: Accuracy = 0.6429600932437491\n",
      "\n",
      "Data shuffled. Epoch:  249\n",
      "Performance on training data: Loss = 0.3796699643135071: Accuracy = 0.846666693687439\n",
      "Performance on test set: : Loss = 5.672164440155029: Accuracy = 0.6430902015764097\n",
      "\n",
      "Optimisation finished!\n"
     ]
    }
   ],
   "source": [
    "%run trainRNN_utils.py\n",
    "%run trainRNN_network_utils.py\n",
    "\n",
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "F1_scores = {}\n",
    "trainAccuracy = {}\n",
    "attention_matrixA = {}\n",
    "attention_matrixB = {}\n",
    "tst_prediction = {}\n",
    "\n",
    "# Create network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Gene A and Gene B input and output placeholders\n",
    "## Input placeholders\n",
    "with tf.variable_scope('geneA'):\n",
    "\n",
    "    rSnpRnaA_pXNS = tf.placeholder(tf.float32, shape = [None, iNnum_H + 1, iSnum_A])\n",
    "\n",
    "    hidden_output_A, current_state_A = dynamicLSTM_Attention(rSnpRnaA_pXNS, \n",
    "                                                                 n_layer, \n",
    "                                                                 n_hidden, \n",
    "                                                                 dropout)\n",
    "\n",
    "hidden_state_A = current_state_A[-1].h\n",
    "\n",
    "with tf.variable_scope('geneB'):\n",
    "\n",
    "    rSnpRnaB_pXNS = tf.placeholder(tf.float32, shape = [None, iNnum_H + 1, iSnum_A])\n",
    "\n",
    "    hidden_output_B, current_state_B = dynamicLSTM_Attention(rSnpRnaB_pXNS, \n",
    "                                                                 n_layer, \n",
    "                                                                 n_hidden, \n",
    "                                                                 dropout)\n",
    "\n",
    "hidden_state_B = current_state_B[-1].h\n",
    "\n",
    "rRelated_pXC = tf.placeholder(tf.float32, \n",
    "                              shape = [None, iCnum_A],\n",
    "                              name = 'rRelated_pXC')\n",
    "\n",
    "context_vectorA, attention_weightsA = attention(hidden_state_A, hidden_output_A, n_hidden)\n",
    "context_vectorB, attention_weightsB = attention(hidden_state_B, hidden_output_B, n_hidden)\n",
    "\n",
    "encoding = tf.concat((context_vectorA, context_vectorB), axis=1)\n",
    "\n",
    "# Dense Layer\n",
    "logits = tf.layers.dense(encoding,\n",
    "                        units = n_classes, \n",
    "                        activation = None,\n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(0.4),\n",
    "                        kernel_initializer = tf.initializers.random_normal() )\n",
    "\n",
    "prediction = tf.argmax(logits, 1)\n",
    "\n",
    "l2 = lambda_l2_reg * sum(\n",
    "    tf.nn.l2_loss(tf_var)\n",
    "        for tf_var in tf.trainable_variables()\n",
    "        if not (\"bias\" in tf_var.name))\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, \n",
    "                                                                     labels=tf.argmax(rRelated_pXC,1)) + l2)\n",
    "\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy; precision, and recall for f1 score\n",
    "correct_pred = tf.equal(prediction, tf.argmax(rRelated_pXC,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Precision and recall\n",
    "rec, rec_op = tf.metrics.recall(labels = tf.argmax(rRelated_pXC, 1), predictions = prediction)\n",
    "pre, pre_op = tf.metrics.precision(labels = tf.argmax(rRelated_pXC, 1), predictions = prediction)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    # Train the network \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    train_f1_score = [None] * n_epoch\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    test_f1_score = []\n",
    "\n",
    "    # Reshape rSnpRnaA_tst_nXSN_H, rRnaA_tst_nXS_H,  rSnpRnaB_tst_nXSN_H, and rRnaB_tst_nXS_H before \n",
    "    # feeding it to the network ( Reason: iSnum_A = 206, iSnum_H = 434). For HBTRC data, randomly select 206\n",
    "    # subjects to align iSnum.\n",
    "    rand_iSnum = np.random.permutation(iSnum_H)[0:206] \n",
    "    rSnpA_tr_nXSN_H_2 = rSnpA_tr_nXSN_H[:, rand_iSnum, :]\n",
    "    rRnaA_tr_nXS_H_2 = rRnaA_tr_nXS_H[:, rand_iSnum]\n",
    "    rSnpB_tr_nXSN_H_2 = rSnpB_tr_nXSN_H[:, rand_iSnum, :]\n",
    "    rRnaB_tr_nXS_H_2 = rRnaB_tr_nXS_H[:, rand_iSnum]\n",
    "\n",
    "    # Reshape and retrive the merged training and test data\n",
    "    rSnpRnaA_tr_nXNS = input_reshape(rSnpA_tr_nXSN_H_2, rRnaA_tr_nXS_H_2)\n",
    "    rSnpRnaB_tr_nXNS = input_reshape(rSnpB_tr_nXSN_H_2, rRnaB_tr_nXS_H_2)\n",
    "    rSnpRnaA_tst_nXNS = input_reshape(rSnpA_tst_nXSN_A, rRnaA_tst_nXS_A)\n",
    "    rSnpRnaB_tst_nXNS = input_reshape(rSnpB_tst_nXSN_A, rRnaB_tst_nXS_A)\n",
    "\n",
    "    for epoch_idx in range(n_epoch): \n",
    "\n",
    "\n",
    "        print(\"Data shuffled.\" + \\\n",
    "              \" Epoch: \", epoch_idx)\n",
    "\n",
    "        # Shuffle classes\n",
    "        rSnpRnaA_tr_nXNS, rSnpRnaB_tr_nXNS = shuffle_classes(rSnpRnaA_tr_nXNS, rSnpRnaB_tr_nXNS)\n",
    "\n",
    "        for batch_idx in range(n_batch):\n",
    "\n",
    "            batch_rSnpRnaA_tXNS = extract_batch_size(rSnpRnaA_tr_nXNS, batch_idx, batch_size)\n",
    "            batch_rSnpRnaB_tXNS = extract_batch_size(rSnpRnaB_tr_nXNS, batch_idx, batch_size)\n",
    "            batch_rRelated_tXC = extract_batch_size(rRelated_tr_nXC_H, batch_idx, batch_size)\n",
    "\n",
    "            # Fit training data\n",
    "            opt, tr_loss, tr_acc = sess.run(\n",
    "                [optimiser, cost, accuracy], \n",
    "                feed_dict = {\n",
    "                    rSnpRnaA_pXNS: batch_rSnpRnaA_tXNS,\n",
    "                    rSnpRnaB_pXNS: batch_rSnpRnaB_tXNS,\n",
    "                    rRelated_pXC: batch_rRelated_tXC                      \n",
    "                })\n",
    "\n",
    "            tst_loss, tst_pre, _, tst_rec, _ = sess.run(\n",
    "                [cost, pre, pre_op, rec, rec_op],\n",
    "                feed_dict = {\n",
    "                    rSnpRnaA_pXNS: rSnpRnaA_tst_nXNS,\n",
    "                    rSnpRnaB_pXNS: rSnpRnaB_tst_nXNS,\n",
    "                    rRelated_pXC: rRelated_tst_nXC_A\n",
    "                    })            \n",
    "\n",
    "\n",
    "            if batch_idx == (n_batch - 1):\n",
    "\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accuracies.append(tr_acc)\n",
    "\n",
    "                tst_f1_score = 2 * ( tst_rec * tst_pre ) / (tst_rec + tst_pre) \n",
    "\n",
    "                test_losses.append(tst_loss)\n",
    "                test_f1_score.append(tst_f1_score)\n",
    "\n",
    "        print(\"Performance on training data\" + \n",
    "             \": Loss = {}\".format(tr_loss) + \n",
    "             \": Accuracy = {}\".format( tr_acc ) )\n",
    "\n",
    "        print(\"Performance on test set: \" + \n",
    "              \": Loss = {}\".format(tst_loss) + \n",
    "              \": Accuracy = {}\".format(tst_f1_score) )\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        if epoch_idx == (n_epoch-1):\n",
    "\n",
    "            for i in range(rSnpRnaA_tst_nXNS.shape[0]):\n",
    "                rSnpRnaA_tst_nXNSA = np.expand_dims(rSnpRnaA_tst_nXNS[i], axis=0)\n",
    "                rSnpRnaB_tst_nXNSB = np.expand_dims(rSnpRnaB_tst_nXNS[i], axis=0)\n",
    "                rRelated_tst_nXC_ = np.expand_dims(rRelated_tst_nXC_A[i], axis=0)\n",
    "\n",
    "                pred, at_weightA, at_weightB = sess.run(\n",
    "                    [prediction, attention_weightsA, attention_weightsB],\n",
    "                    feed_dict = {\n",
    "                            rSnpRnaA_pXNS: rSnpRnaA_tst_nXNSA,\n",
    "                            rSnpRnaB_pXNS: rSnpRnaB_tst_nXNSB,\n",
    "                            rRelated_pXC: rRelated_tst_nXC_\n",
    "                            }) \n",
    "\n",
    "                at_weightA = np.reshape(at_weightA, (-1, 1))\n",
    "                at_weightB = np.reshape(at_weightB, (-1, 1))\n",
    "\n",
    "                attention_matrixA[dropout] = at_weightA\n",
    "                attention_matrixB[dropout] = at_weightB                    \n",
    "                tst_prediction[dropout] = pred\n",
    "\n",
    "    trainLosses[dropout] = train_losses\n",
    "    testLosses[dropout] = test_losses\n",
    "    trainAccuracy[dropout] = train_accuracies\n",
    "    F1_scores[dropout] = test_f1_score\n",
    "    print(\"Optimisation finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run trainRNN_plot_utils.py\n",
    "plot_one_input(F1_scores, trainLosses, testLosses, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved into pickle.\n"
     ]
    }
   ],
   "source": [
    "# SAVE DATA\n",
    "# Save the created samples, such tha the NNs can load them easily\n",
    "\n",
    "# Save data into Python friendly file\n",
    "import pickle\n",
    "with open('resultsAttention_HBTRC_ANM.pickle', 'wb') as f:\n",
    "    pickle.dump( rSnpRnaA_tst_nXNS, f, pickle.HIGHEST_PROTOCOL )\n",
    "    pickle.dump( rSnpRnaB_tst_nXNS, f, pickle.HIGHEST_PROTOCOL )\n",
    "    pickle.dump( trainLosses, f, pickle.HIGHEST_PROTOCOL )\n",
    "    pickle.dump( testLosses, f, pickle.HIGHEST_PROTOCOL )\n",
    "    pickle.dump( F1_scores, f, pickle.HIGHEST_PROTOCOL )\n",
    "    pickle.dump( trainAccuracy, f, pickle.HIGHEST_PROTOCOL )\n",
    "    pickle.dump( attention_matrixA, f, pickle.HIGHEST_PROTOCOL )\n",
    "    pickle.dump( attention_matrixB, f, pickle.HIGHEST_PROTOCOL )\n",
    "    pickle.dump( tst_prediction, f, pickle.HIGHEST_PROTOCOL )\n",
    "    print( 'Data saved into pickle.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
